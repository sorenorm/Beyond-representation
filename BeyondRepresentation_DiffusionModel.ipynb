{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diffusion model based on https://github.com/iconvk/LearningIndependentCascadeOnVK repurposed for the ParlaMint data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the ParlaMint data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = r'C:\\Users\\soren\\Documents\\beyond_rep\\parlamint_raw'\n",
    "folders = [x[0] for x in os.walk(dir_path) if x[0][-8:-5] == 'txt']\n",
    "\n",
    "\n",
    "df = pd.DataFrame()\n",
    "counter = 1\n",
    "for folder in folders:\n",
    "    df_country = pd.DataFrame()\n",
    "\n",
    "    os.chdir(folder)\n",
    "    \n",
    "    tsv_files = glob.glob('*meta-en.tsv')\n",
    "\n",
    "    for file in tsv_files:\n",
    "        path = folder + '/' + file\n",
    "        d_temp = pd.read_csv(path, sep='\\t')\n",
    "\n",
    "        try:\n",
    "            df_text = pd.read_csv(path[:-12] + '.txt', sep = '\\t', header = None).rename(columns={0:'ID', 1:'text'})\n",
    "\n",
    "            d_temp['text'] = df_text['text'] \n",
    "        except:\n",
    "            print('failed... ' + path[:-12] + '.txt')\n",
    "            pass\n",
    "\n",
    "        df_country = df_country._append(d_temp)\n",
    "\n",
    "    if len(folder) == 71:\n",
    "        country = folder[-11:-9]\n",
    "    else:\n",
    "        country = folder[-15:-10]\n",
    "\n",
    "    print(f'{country}: {counter} of {len(folders)}')\n",
    "    counter += 1\n",
    "\n",
    "    df_country = df_country.assign(country = country)\n",
    "\n",
    "    df_country.to_csv(f\"parlamint_csv/{country}.csv\")\n",
    "\n",
    "    df = df._append(df_country)\n",
    "\n",
    "\n",
    "df['Speaker_birth'] = [str(i) if i != '-' else np.nan for i in df.Speaker_birth]\n",
    "\n",
    "df = df.assign(date = [datetime.strptime(i, '%Y-%m-%d') for i in df['From']],\n",
    "                    Speaker_birth = [datetime.strptime(i, '%Y') if i != 'nan' else i for i in df['Speaker_birth']])\n",
    "\n",
    "df['Speaker_age'] = [i if pd.isna(i) else round(int(i)/365) for i in ((df.date - df.Speaker_birth).dt.days)]\n",
    "\n",
    "df['year'] = [int(i[0:4]) for i in df.From]\n",
    "\n",
    "df['gender'] = [0 if i == 'M' else 1 for i in df.Speaker_gender]\n",
    "\n",
    "df.to_csv(\"PATH_TO/ParlaMint_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the diffusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import itertools\n",
    "\n",
    "import pickle as pkl\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "from multiprocessing import Pool\n",
    "from collections import Counter\n",
    "from math import log\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "\n",
    "import pickle\n",
    "from sklearn import metrics\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch_directml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_network_year(data, country, house):\n",
    "    G = nx.DiGraph()\n",
    "    nodes = pd.DataFrame(data[['Speaker_name', 'Speaker_minister', 'Speaker_party', 'Speaker_gender', 'Speaker_birth', 'Party_status']].value_counts().reset_index())\n",
    "    # adding nodes\n",
    "    for _, row in nodes.iterrows():\n",
    "        speaker = row['Speaker_name']\n",
    "        age = row['Speaker_birth']\n",
    "        gender = row['Speaker_gender']\n",
    "        party = row['Speaker_party']\n",
    "        party_status = row['Party_status']\n",
    "        minister = row['Speaker_minister']\n",
    "        # Check if speaker node already exists in the graph\n",
    "        if speaker not in G.nodes():\n",
    "            G.add_node(speaker, age=age, gender=gender, \n",
    "                   party=party, party_status = party_status, minister = minister)\n",
    "    # adding edges\n",
    "    for date in set(data['dateRank'].to_list()):\n",
    "        speakers_on_same_day = data[data['dateRank'] == date]['Speaker_name'].tolist()\n",
    "        for edge in itertools.combinations(speakers_on_same_day, 2):\n",
    "            if edge[0] != edge[1]:\n",
    "                edge = sorted(edge)\n",
    "                if G.has_edge(edge[0], edge[1]):\n",
    "                    G[edge[0]][edge[1]]['weight'] += 1\n",
    "                else:\n",
    "                    G.add_edge(edge[0], edge[1], weight=1)\n",
    "    with open(f'yearWise\\\\network{country}{house}{max(data.year)}{min(data.year)}.pkl', 'wb') as f:\n",
    "        pickle.dump(G, f, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_relevant_nodes_year(data, country, house):\n",
    "    with open(f'yearWise\\\\network{country}{house}{max(data.year)}{min(data.year)}.pkl', 'rb') as f:\n",
    "        G = pickle.load(f)\n",
    "    a = data.groupby('Speaker_name')['text'].count()\n",
    "    a = a[a >= 5]\n",
    "    valid_nodes = []\n",
    "    for n in tqdm(G.nodes()):\n",
    "        if n in a.index:\n",
    "            valid_nodes.append(n)\n",
    "    g = G.subgraph(valid_nodes).to_undirected() \n",
    "    g = g.subgraph(next(nx.connected_components(g))) \n",
    "    with open(f'yearWise\\\\filtered_network{country}{house}{max(data.year)}{min(data.year)}.pkl', 'wb') as f:\n",
    "        pickle.dump(g, f, pickle.HIGHEST_PROTOCOL)\n",
    "    posts_in_graph = pd.DataFrame(data[data['Speaker_name'].isin(g)])\n",
    "    posts_in_graph = posts_in_graph[['Speaker_name', 'timeOfDay', 'dateRank', 'text']].reset_index()\n",
    "    pkl.dump(posts_in_graph, open(f'yearWise\\\\new_collected_posts_{country}{house}{max(data.year)}{min(data.year)}.pkl', \"wb\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_document_pool(embeddings):\n",
    "    # Ensure at least one embedding is provided\n",
    "    if len(embeddings) == 0:\n",
    "        raise ValueError(\"Empty list of embeddings\")\n",
    "\n",
    "    # Calculate the mean embedding\n",
    "    mean_embedding = np.mean([i for i in embeddings], axis=0)\n",
    "    return mean_embedding\n",
    "\n",
    "def recursive_split(string, embedding_model):\n",
    "\n",
    "    if len(embedding_model.tokenizer(string)['input_ids']) <= 512:\n",
    "        return [string]\n",
    "\n",
    "    # Find the index of the middle full stop\n",
    "    full_stops = [i for i, char in enumerate(string) if char in ['.', '!', '?']]\n",
    "\n",
    "    if not full_stops:\n",
    "        full_stops = [i for i, char in enumerate(string) if char in ['/', ',', ':', ';', '(', ')']]\n",
    "\n",
    "    middle_index = len(full_stops) // 2\n",
    "\n",
    "    if len(full_stops) % 2 == 0:  # Even number of full stops\n",
    "        # Take the one before the midpoint\n",
    "        middle_index -= 1\n",
    "    # Split the string using the middle full stop index\n",
    "    first_part = string[:full_stops[middle_index]]\n",
    "    second_part = string[full_stops[middle_index] + 1:]\n",
    "\n",
    "    if (len(embedding_model.tokenizer(first_part)['input_ids']) <= 512) and (\n",
    "            len(embedding_model.tokenizer(second_part)['input_ids']) <= 512):\n",
    "        return [first_part, second_part]\n",
    "    else:\n",
    "        # Recursively split the parts that are too long\n",
    "        first_part_splits = recursive_split(first_part, embedding_model)\n",
    "        second_part_splits = recursive_split(second_part, embedding_model)\n",
    "        return first_part_splits + second_part_splits\n",
    "\n",
    "\n",
    "def getSpeechEmbd(speech, model):\n",
    "    speech_split = recursive_split(speech, model)\n",
    "    sentence_embeddings = []\n",
    "    for part in speech_split:\n",
    "        sentence_embedding = model.encode(part)\n",
    "        sentence_embeddings.append(sentence_embedding)\n",
    "\n",
    "    speech_embedding = mean_document_pool(sentence_embeddings)\n",
    "    return speech_embedding\n",
    "\n",
    "def are_posts_close(th, post1_emb, post2_emb):\n",
    "    \n",
    "    c = 1.-cosine(post1_emb, post2_emb) # cosine here is 1-cos (scipy)\n",
    "    return [c >= t for t in th]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech2vec_year(country, year):\n",
    "    device = torch_directml.device()\n",
    "\n",
    "    model = SentenceTransformer('intfloat/multilingual-e5-base', device = device)\n",
    "\n",
    "    posts_in_graph = pkl.load(open(f'yearWise\\\\new_collected_posts_{country}{year}.pkl', \"rb\"))\n",
    "\n",
    "    speechVectors = []\n",
    "\n",
    "    for speech in tqdm(posts_in_graph.text):\n",
    "        if isinstance(speech, str):\n",
    "            speechVectors.append(getSpeechEmbd(speech, model))\n",
    "        else:\n",
    "            speechVectors.append(getSpeechEmbd('(...)', model))\n",
    "\n",
    "    posts_in_graph['speechVector'] = speechVectors\n",
    "    pkl.dump(posts_in_graph, open(f'yearWise\\\\speechVectors{country}{year}.pkl', \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_Av2u_year(data, country, house):\n",
    "    posts_in_graph = pkl.load(open(f'yearWise\\\\speechVectors{country}{house}{max(data.year)}{min(data.year)}.pkl', \"rb\"))\n",
    "    with open(f'yearWise\\\\filtered_network{country}{house}{max(data.year)}{min(data.year)}.pkl', 'rb') as f:\n",
    "        G_with_posts = pickle.load(f)\n",
    "    train_posts_in_graph, test_posts_in_graph = train_test_split(posts_in_graph, test_size=0.3)\n",
    "    train_posts_in_graph.to_pickle(f'yearWise\\\\train_posts_in_graph{country}{house}{max(data.year)}{min(data.year)}.pkl')\n",
    "    test_posts_in_graph.to_pickle(f'yearWise\\\\test_posts_in_graph{country}{house}{max(data.year)}{min(data.year)}.pkl')\n",
    "    Av2u = []\n",
    "    already_success = []\n",
    "    for _ in range(len(th)): # tau cut-offs\n",
    "        Av2u.append(Counter())\n",
    "        already_success.append(set())\n",
    "    published = defaultdict(lambda: [])\n",
    "    time_threshold = 3\n",
    "    train_posts_in_graph = train_posts_in_graph.sort_values([\"dateRank\", 'timeOfDay'])\n",
    "    for post_ind in tqdm(train_posts_in_graph.index):\n",
    "        post = train_posts_in_graph.loc[post_ind]\n",
    "        timeOfDay, time, post1embd, post1, u = post['timeOfDay'], post['dateRank'], post['speechVector'], post['text'], post['Speaker_name']\n",
    "        for neighbor in G_with_posts.neighbors(u):\n",
    "            forbidden_i = set() # forbid for u to be influenced by neighbor for post, if it was already influenced by that neighbor on that post\n",
    "            for post2, post2embd, time2, timeOfDay2 in published[neighbor]:\n",
    "                if time - time2 > time_threshold:\n",
    "                    break\n",
    "                if (time == time2) & (timeOfDay > timeOfDay2):\n",
    "                    break\n",
    "                try:\n",
    "                    th_succ = are_posts_close(th, post1embd, post2embd)\n",
    "                    for i in range(len(th)):\n",
    "                        if th_succ[i]:\n",
    "                            if ((post2, u) not in already_success[i]) and (i not in forbidden_i):\n",
    "                                Av2u[i][(neighbor,u)] += 1\n",
    "                                already_success[i].add((post2, u)) # owner_id can not publish more copies of similar post like post_id2\n",
    "                                forbidden_i.add(i)\n",
    "                except:\n",
    "                    pass\n",
    "        published[u] = [(post1, post1embd, time, timeOfDay)] + published[u]\n",
    "    pkl.dump(Av2u, open(f'yearWise\\\\Av2u{country}{house}{max(data.year)}{min(data.year)}.pkl', \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_probabilities_to_edges_year(data, country, house):\n",
    "    #print(year)\n",
    "    Av2u = pkl.load(open(f'yearWise\\\\Av2u{country}{house}{max(data.year)}{min(data.year)}.pkl', \"rb\"))\n",
    "    with open(f'yearWise\\\\filtered_network{country}{house}{max(data.year)}{min(data.year)}.pkl', 'rb') as f:\n",
    "        G_with_posts = pickle.load(f)\n",
    "    posts_in_graph = pkl.load(open(f'yearWise\\\\new_collected_posts_{country}{house}{max(data.year)}{min(data.year)}.pkl', \"rb\"))\n",
    "    weighted_Gs = []\n",
    "    Au = posts_in_graph.groupby(\"Speaker_name\")['text'].count()\n",
    "    for th_index in tqdm(range(len(th))):\n",
    "        all_edges = []\n",
    "        for e in G_with_posts.edges():\n",
    "            if ((e[0], e[1]) in Av2u[th_index]):\n",
    "                all_edges.append((e[0], e[1], Av2u[th_index][(e[0], e[1])]/Au[e[0]]))\n",
    "            if (e[1], e[0]) in Av2u[th_index]:\n",
    "                all_edges.append((e[1], e[0], Av2u[th_index][(e[1], e[0])]/Au[e[1]]))\n",
    "        weighted_Gs.append(nx.DiGraph())\n",
    "        weighted_Gs[-1].add_weighted_edges_from(all_edges)\n",
    "    pkl.dump(weighted_Gs, open(f'yearWise\\\\weighted_Gs{country}{house}{max(data.year)}{min(data.year)}.pkl', \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prediction_year(data, country, house):\n",
    "    with open(f'yearWise\\\\filtered_network{country}{house}{max(data.year)}{min(data.year)}.pkl', 'rb') as f:\n",
    "        G_with_posts = pickle.load(f)\n",
    "    weighted_Gs = pkl.load(open(f'yearWise\\\\weighted_Gs{country}{house}{max(data.year)}{min(data.year)}.pkl', \"rb\"))\n",
    "    test_posts_in_graph = pkl.load(open(f'yearWise\\\\test_posts_in_graph{country}{house}{max(data.year)}{min(data.year)}.pkl', 'rb'))\n",
    "    test_posts_in_graph = test_posts_in_graph.sort_values([\"dateRank\", 'timeOfDay'])\n",
    "    G_with_posts_directed = G_with_posts.to_directed()\n",
    "    for e in G_with_posts_directed.edges():\n",
    "        G_with_posts_directed[e[0]][e[1]]['success'] = [0]*len(th)\n",
    "        G_with_posts_directed[e[0]][e[1]]['failure'] = [0]*len(th)\n",
    "    published = defaultdict(lambda: [])\n",
    "    posts_that_has_been_reposted = set() # tuples <source post, target_user, threshold>\n",
    "    time_threshold = 3\n",
    "    for post_index in tqdm(test_posts_in_graph.index):\n",
    "        post = test_posts_in_graph.loc[post_index]\n",
    "        timeOfDay, time, post1embd, post1, u = post['timeOfDay'], post['dateRank'], post['speechVector'], post['text'], post['Speaker_name']\n",
    "        for neighbor in G_with_posts.neighbors(u):\n",
    "            thresholds_where_neighbor_have_influenced = set()\n",
    "            for post2, post2embd, time2, timeOfDay2 in published[neighbor]:\n",
    "                if (time - time2 > time_threshold) or (len(thresholds_where_neighbor_have_influenced) == len(th)):\n",
    "                    break # other posts of neighbor are either too old, or already influenced u on the post\n",
    "                try:\n",
    "                    th_succ = are_posts_close(th, post1embd, post2embd)\n",
    "                    for i in range(len(th)):\n",
    "                        if th_succ[i] and ((post2, u, i) not in posts_that_has_been_reposted) and (i not in thresholds_where_neighbor_have_influenced):\n",
    "                            posts_that_has_been_reposted.add((post2, u, i))\n",
    "                            G_with_posts_directed[neighbor][u]['success'][i] += 1\n",
    "                            G_with_posts_directed[neighbor][u]['failure'][i] -= 1\n",
    "                            assert(G_with_posts_directed[neighbor][u]['failure'][i] >= 0)\n",
    "                            thresholds_where_neighbor_have_influenced.add(i)\n",
    "                except:\n",
    "                    pass\n",
    "        published[u] = [(post1, post1embd, time, timeOfDay)] + published[u]\n",
    "        for n in G_with_posts_directed.successors(u):\n",
    "            for i in range(len(th)):\n",
    "                G_with_posts_directed[u][n]['failure'][i] += 1\n",
    "    rates = []\n",
    "    for i in tqdm(range(len(th))):\n",
    "        rates.append([])\n",
    "        for e in G_with_posts_directed.edges(data=True):\n",
    "            if weighted_Gs[i].has_edge(e[0], e[1]):\n",
    "                prob = weighted_Gs[i][e[0]][e[1]]['weight']\n",
    "            else:\n",
    "                prob = 0\n",
    "            if e[2]['success'][i] > 0 or e[2]['failure'][i] > 0:\n",
    "                rates[-1].append((e[2]['success'][i], e[2]['failure'][i], prob))\n",
    "    pkl.dump(rates, open(f'yearWise\\\\rates{country}{house}{max(data.year)}{min(data.year)}.pkl', \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tpr_fpr(data):\n",
    "    sorted_data = sorted(data, key=lambda x: -x[2])  # Sort based on edge probabilities\n",
    "    total_positives = sum([x[0] for x in sorted_data])\n",
    "    total_negatives = sum([x[1] for x in sorted_data])\n",
    "    tp, fp = 0, 0\n",
    "    tpr_list, fpr_list = [0], [0]\n",
    "\n",
    "    for successes, failures, _ in sorted_data:\n",
    "        tp += successes\n",
    "        fp += failures\n",
    "        if total_positives != 0:\n",
    "            tpr = tp / total_positives\n",
    "        else:\n",
    "            tpr = 0\n",
    "        if total_negatives != 0:\n",
    "            fpr = fp / total_negatives\n",
    "        else:\n",
    "            fpr = 0\n",
    "        tpr_list.append(tpr)\n",
    "        fpr_list.append(fpr)\n",
    "\n",
    "    return tpr_list, fpr_list\n",
    "\n",
    "\n",
    "def calculate_roc_curve(data, country, house):\n",
    "    best_thresholds = []\n",
    "    auc_values = []\n",
    "    for i, threshold in tqdm(enumerate(th)):\n",
    "        rates = pkl.load(open(f'yearWise\\\\rates{country}{house}{max(data.year)}{min(data.year)}.pkl', \"rb\"))\n",
    "        rateList = rates[i]\n",
    "        tpr, fpr = get_tpr_fpr(rateList)\n",
    "        auc = metrics.auc(tpr, fpr)\n",
    "        auc_values.append(auc)\n",
    "    max_auc_index = np.argmax(auc_values)\n",
    "    best_threshold = th[max_auc_index]\n",
    "    best_thresholds.append(best_threshold)\n",
    "    best_auc = auc_values[max_auc_index]\n",
    "    plt.plot(th, auc_values)#, marker='o')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.title(f'{country} Area Under the ROC Curve (AUC) for Different Thresholds')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    print(f\"Best threshold value: {best_threshold}\")\n",
    "    print(f\"Highest AUC: {best_auc}\")\n",
    "\n",
    "    return best_thresholds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_with_best_th_year(data, country, house, best_threshold):\n",
    "    \n",
    "    posts_in_graph = pkl.load(open(f'yearWise\\\\speechVectors{country}{house}{max(data.year)}{min(data.year)}.pkl', \"rb\"))\n",
    "    with open(f'yearWise\\\\filtered_network{country}{house}{max(data.year)}{min(data.year)}.pkl', 'rb') as f:\n",
    "        G_with_posts = pickle.load(f)\n",
    "    Av2u = []\n",
    "    already_success = []\n",
    "    for _ in [best_threshold]:\n",
    "        Av2u.append(Counter())\n",
    "        already_success.append(set())\n",
    "    published = defaultdict(lambda: [])\n",
    "    time_threshold = 3\n",
    "    posts_in_graph = posts_in_graph.sort_values([\"dateRank\", 'timeOfDay'])\n",
    "    for post_ind in tqdm(posts_in_graph.index):\n",
    "        post = posts_in_graph.loc[post_ind]\n",
    "        timeOfDay, time, post1embd, post1, u = post['timeOfDay'], post['dateRank'], post['speechVector'], post['text'], post['Speaker_name']\n",
    "        for neighbor in G_with_posts.neighbors(u):\n",
    "            forbidden_i = set() # forbid for u to be influenced by neighbor for post, if it was already influenced by that neighbor on that post\n",
    "            for post2, post2embd, time2, timeOfDay2 in published[neighbor]:\n",
    "                if time - time2 > time_threshold:\n",
    "                    break\n",
    "                if (time == time2) & (timeOfDay > timeOfDay2):\n",
    "                    break\n",
    "                try:\n",
    "                    th_succ = are_posts_close(th, post1embd, post2embd)\n",
    "                    for i in range(len([best_threshold])):\n",
    "                        if th_succ[i]:\n",
    "                            if ((post2, u) not in already_success[i]) and (i not in forbidden_i):\n",
    "                                Av2u[i][(neighbor,u)] += 1\n",
    "                                already_success[i].add((post2, u)) # owner_id can not publish more copies of similar post like post_id2\n",
    "                                forbidden_i.add(i)\n",
    "                except:\n",
    "                    pass\n",
    "        published[u] = [(post1, post1embd, time, timeOfDay)] + published[u]\n",
    "    weighted_Gs = []\n",
    "    Au = posts_in_graph.groupby(\"Speaker_name\")['text'].count()\n",
    "    for th_index in tqdm(range(len([best_threshold]))):\n",
    "        all_edges = []\n",
    "        for e in G_with_posts.edges():\n",
    "            if ((e[0], e[1]) in Av2u[th_index]):\n",
    "                all_edges.append((e[0], e[1], Av2u[th_index][(e[0], e[1])]/Au[e[0]]))\n",
    "            if (e[1], e[0]) in Av2u[th_index]:\n",
    "                all_edges.append((e[1], e[0], Av2u[th_index][(e[1], e[0])]/Au[e[1]]))\n",
    "        weighted_Gs.append(nx.DiGraph())\n",
    "        weighted_Gs[-1].add_weighted_edges_from(all_edges)\n",
    "    pkl.dump(weighted_Gs, open(f'yearWise\\\\weighted_Gs_retrained{country}{house}{max(data.year)}{min(data.year)}.pkl', \"wb\"))\n",
    "    pkl.dump(all_edges, open(f'yearWise\\\\all_edges{country}{house}{max(data.year)}{min(data.year)}.pkl', \"wb\"))\n",
    "\n",
    "#retrain_with_best_th_year(df, country, house, best_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "countryHouseList = [('AT', 'all'), \n",
    "                    ('BA', 'all'), \n",
    "                    ('BE', 'com'),\n",
    "                    ('BE', 'low'),\n",
    "                    ('BG', 'all'), \n",
    "                    ('CZ', 'all'), \n",
    "                    ('DK', 'all'), \n",
    "                    ('EE', 'all'), \n",
    "                    ('ES', 'all'), \n",
    "                    ('ES-CT', 'all'), \n",
    "                    ('ES-GA', 'all'), \n",
    "                    ('ES-PV', 'all'), \n",
    "                    ('FI', 'all'), \n",
    "                    ('FR', 'all'),  \n",
    "                    ('GB', 'low'), \n",
    "                    ('GB', 'upp'), \n",
    "                    ('GR', 'all'), \n",
    "                    ('HR', 'all'), \n",
    "                    ('HU', 'all'), \n",
    "                    ('IS', 'all'), \n",
    "                    ('IT', 'all'), \n",
    "                    ('LV', 'all'), \n",
    "                    ('NL', 'low'), \n",
    "                    ('NL', 'upp'), \n",
    "                    ('NO', 'all'), \n",
    "                    ('PL', 'low'), \n",
    "                    ('PL', 'upp'), \n",
    "                    ('PT', 'all'), \n",
    "                    ('RS', 'all'),  \n",
    "                    ('SE', 'all'),  \n",
    "                    ('SI', 'all'),  \n",
    "                    ('TR', 'all'), \n",
    "                    ('UA', 'all') \n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "elections = {'ATall':['01-01-1996', '03/10/1999', '24/11/2002', '01/10/2006', '28/09/2008', '29/09/2013', '15/10/2017', '29/09/2019', '16/09/2024'],\n",
    "             'BAall':['01-01-1996', '05/10/2002', '01-10-2006','03-10-2010','12-10-2014','07-10-2018','02-10-2022','','16/09/2024'],\n",
    "             'BEcom':['01-01-1996', '25-05-2014', '26-05-2019', '16/09/2024'],\n",
    "             'BElow':['01-01-1996', '25-05-2014', '26-05-2019', '16/09/2024'],\n",
    "             'BGall':['01-01-1996', '05-10-2014', '26-03-2017', '04-04-2021', '11-07-2021', '21-11-2021', '16/09/2024'],\n",
    "             'CZall':['01-01-1996', '25-10-2013', '20-10-2017', '08-10-2021', '16/09/2024'],\n",
    "             'DKall':['01-01-1996', '18-06-2015', '05-06-2019', '16/09/2024'],\n",
    "             'EEall':['01-01-1996', '06-03-2011', '01-03-2015', '03-03-2019', '16/09/2024'],\n",
    "             'ESall':['01-01-1996', '20-12-2015', '26-06-2016', '28-04-2019', '10-11-2019', '23-07-2023', '16/09/2024'],\n",
    "             'ES-CTall':['01-01-1996', '27-09-2015', '21-12-2017', '14-02-2021', '16/09/2024'],\n",
    "             'ES-GAall':['01-01-1996', '25-09-2016', '12-07-2020', '16/09/2024'],\n",
    "             'ES-PVall':['01-01-1996', '25-09-2016', '12/07/2020', '16/09/2024'],\n",
    "             'FIall':['01-01-1996', '19-04-2015', '14-04-2019', '16/09/2024'],\n",
    "             'FRall':['01-01-1996', '18-06-2017', '19-06-2022', '16/09/2024'],\n",
    "             'GBlow':['01-01-1996', '06-01-2016', '06-01-2017', '06-01-2018', '12-12-2019', '06-01-2020', '06-01-2021', '16/09/2024'],\n",
    "             'GBupp':['01-01-1996', '16/09/2016', '16/09/2018', '16/09/2020', '16/09/2022', '16/09/2024'],\n",
    "             'GRall':['01-01-1996', '25-01-2015', '20-09-2015', '07-07-2019', '16/09/2024'],\n",
    "             'HRall':['01-01-1996', '25-11-2007', '04-12-2011', '08-11-2015', '11-09-2016', '05-07-2020', '16/09/2024'],\n",
    "             'HUall':['01-01-1996', '06-04-2014', '08-04-2018', '03-04-2022', '16/09/2024'],\n",
    "             'ISall':['01-01-1996', '29/10/2016', '28-10-2017', '25-09-2021', '16/09/2024'],\n",
    "             'ITall':['01-01-1996', '24-02-2013', '04-03-2018', '25-09-2022', '16/09/2024'],\n",
    "             'LVall':['01-01-1996', '04-10-2014', '01-10-2018', '01-10-2022', '16/09/2024'],\n",
    "             'NLlow':['01-01-1996', '12-09-2012', '15-03-2017', '17-03-2021', '16/09/2024'],\n",
    "             'NLupp':['01-01-1996', '26-05-2015', '27-05-2019', '16/09/2024'],\n",
    "             'NOall':['01-01-1996', '14-09-2009', '09-09-2013', '11-09-2017', '13-09-2021', '16/09/2024'],\n",
    "             'PLlow':['01-01-1996', '25-10-2015', '13-10-2019', '16/09/2024'],\n",
    "             'PLupp':['01-01-1996', '25-10-2015', '13-10-2019', '16/09/2024'],\n",
    "             'PTall':['01-01-1996', '04-10-2015', '06/10/2019', '30-01-2022', '16/09/2024'],\n",
    "             'RSall':['01-01-1996', '05-10-1997', '23-12-2000', '28-12-2003', '21-01-2007', '11-05-2008', '06-05-2012', '16-03-2014', '24-04-2016', '21-06-2021', '03-04-2022', '16/09/2024'],\n",
    "             'SEall':['01-01-1996', '14/09/2014', '09-09-2018', '11-09-2022', '16/09/2024'],\n",
    "             'SIall':['01-01-1996', '15-10-2000', '03-10-2004', '21-09-2008', '04-12-2011', '13-07-2014', '03-06-2018', '24-04-2022', '16/09/2024'],\n",
    "             'TRall':['01-01-1996', '12-06-2011', '07-06-2015', '01-11-2015', '24-06-2018', '16/09/2024'],\n",
    "             'UAall':['01-01-1996', '28-10-2012', '26-10-2014', '21-07-2019','16/09/2024']\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country, house in countryHouseList:\n",
    "    df = pd.read_csv(f'parlamint_csv\\\\{country}.csv')\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Creating a new column 'date_rank' counting upward\n",
    "    df['timeOfDay'] = df.groupby('Date').cumcount()\n",
    "    df['dateRank'] = df.groupby(df['Date']).ngroup()\n",
    "    df['year'] = [int(i[0:4]) for i in df.Date]\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')\n",
    "\n",
    "    df = df[df['Speaker_role'] != 'Chairperson']\n",
    "\n",
    "    th = list(np.arange(0.7,0.95,0.005)) + list(np.arange(0.95,0.999,0.001))\n",
    "\n",
    "    for i, date in enumerate(elections[f'{country}{house}'][1:]):\n",
    "        print(f'{elections[f'{country}{house}'][i]} - {date}')\n",
    "        df_year = df[(df['Date'] > elections[f'{country}{house}'][i]) & (df['Date'] < date)]\n",
    "\n",
    "        if len(df_year) != 0:\n",
    "\n",
    "            data_to_network_year(df_year, country, house) \n",
    "            filter_relevant_nodes_year(df_year, country, house)\n",
    "            speech2vec_year(df_year, country, house)\n",
    "            calculate_Av2u_year(df_year, country, house)\n",
    "            assign_probabilities_to_edges_year(df_year, country, house)\n",
    "            test_prediction_year(df_year, country, house)\n",
    "            best_threshold = calculate_roc_curve(df_year, country, house)\n",
    "            retrain_with_best_th_year(df_year, country, house, best_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting influences and saving ROC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle as pkl\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_roc_curve(data):\n",
    "    sorted_data = sorted(data, key=lambda x: -x[2])  # Sort based on edge probabilities\n",
    "    #sorted_data = data.sort(key = lambda row: row[2])\n",
    "    total_positives = sum([x[0] for x in sorted_data])\n",
    "    total_negatives = sum([x[1] for x in sorted_data])\n",
    "    tp, fp = 0, 0\n",
    "    tpr_list, fpr_list = [0], [0]\n",
    "\n",
    "    for successes, failures, _ in sorted_data:\n",
    "        tp += successes\n",
    "        fp += failures\n",
    "        if total_positives != 0:\n",
    "            tpr = tp / total_positives\n",
    "        else:\n",
    "            tpr = 0\n",
    "        if total_negatives != 0:\n",
    "            fpr = fp / total_negatives\n",
    "        else:\n",
    "            fpr = 0\n",
    "        tpr_list.append(tpr)\n",
    "        fpr_list.append(fpr)\n",
    "\n",
    "    return tpr_list, fpr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract auc and thresholds\n",
    "\n",
    "os.chdir('PATH_TO\\\\yearWise')\n",
    "ratesFiles = glob.glob('rates*') # weighted Gs\n",
    "\n",
    "#ratesFiles = [i for i in ratesFiles if i[5:7] in ['NL']]\n",
    "\n",
    "th = list(np.arange(0.7, 0.95, 0.005)) + list(np.arange(0.95, 0.999, 0.001))\n",
    "\n",
    "# INITIALIZE EMPTY DATAFRAMES\n",
    "d_ratesBest = pd.DataFrame()\n",
    "d_ratesAll = pd.DataFrame()\n",
    "d_ROC = pd.DataFrame()\n",
    "\n",
    "for ratesFile in tqdm(ratesFiles):\n",
    "    print(ratesFile)\n",
    "    # LOAD RATES DATA FROM FILE\n",
    "    with open(ratesFile, \"rb\") as f:\n",
    "        rates = pkl.load(f)\n",
    "\n",
    "    # COLLECT AUC VALUES FOR EACH THRESHOLD\n",
    "    auc_values = []\n",
    "    fpr_list = []\n",
    "    tpr_list = []\n",
    "\n",
    "    for i, threshold in enumerate(th):\n",
    "        data = rates[i]\n",
    "        tpr, fpr = calculate_roc_curve(data)\n",
    "        fpr_list.append(fpr)\n",
    "        tpr_list.append(tpr)\n",
    "        auc = metrics.auc(fpr, tpr)\n",
    "        auc_values.append(auc)\n",
    "\n",
    "    # FIND THE BEST AUC AND CORRESPONDING THRESHOLD\n",
    "    max_auc_index = np.argmax(auc_values)\n",
    "    best_threshold = th[max_auc_index]\n",
    "    best_auc = auc_values[max_auc_index]\n",
    "\n",
    "    # EXTRACT UPPERCASE LETTERS FROM FILE NAME\n",
    "    country = ''.join([char for char in ratesFile if char.isupper()])\n",
    "    house = ratesFile[-15:-12]\n",
    "    year = ratesFile[-12:-4]\n",
    "\n",
    "    # STORE BEST AUC AND THRESHOLD\n",
    "    dictBest = {'country': country, \n",
    "                'house': house,\n",
    "                'year': year,\n",
    "                'threshold': best_threshold,\n",
    "                'auc': best_auc\n",
    "                }\n",
    "    \n",
    "    # APPEND TO d_ratesBest DATAFRAME\n",
    "    d_ratesBest = d_ratesBest._append(dictBest, ignore_index=True)\n",
    "\n",
    "    # STORE ALL AUC VALUES AND OTHER DETAILS\n",
    "    #d_rateAll = pd.DataFrame({'country': [country] * len(th), \n",
    "    #                          'house': [house] * len(th),\n",
    "    #                          'year': [year] * len(th),\n",
    "    #                          'threshold' : th,\n",
    "    #                          'auc': auc_values\n",
    "    #                          })\n",
    "    downsample_size = 500  # Set the desired sample size here\n",
    "    sampled_indices = pd.Series(range(len(fpr_list[max_auc_index]))).sample(n=downsample_size, random_state=1).tolist()\n",
    "    downsampled_data = pd.DataFrame({\n",
    "        'fpr': [fpr_list[max_auc_index][i] for i in sampled_indices],\n",
    "        'tpr': [tpr_list[max_auc_index][i] for i in sampled_indices],\n",
    "        'country': country,\n",
    "        'year': year,\n",
    "        'house': house})\n",
    "\n",
    "    d_ROC = pd.concat([d_ROC, downsampled_data], ignore_index=True)\n",
    "\n",
    "    # CONCATENATE TO d_ratesAll DATAFRAME\n",
    "    #d_ratesAll = pd.concat([d_ratesAll, d_rateAll], ignore_index=True)\n",
    "\n",
    "\n",
    "d_ratesBest.to_csv('influence\\\\ratesBest.csv')\n",
    "d_ROC.to_csv('influence\\\\ROC.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract influence sum\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\soren\\\\Documents\\\\beyond_rep\\\\yearWise')\n",
    "edgesFiles = glob.glob('all_edges*')\n",
    "\n",
    "influence = pd.DataFrame()\n",
    "\n",
    "for edgesFile in tqdm(edgesFiles):\n",
    "    with open(edgesFile, \"rb\") as f:\n",
    "        edges = pkl.load(f)\n",
    "    \n",
    "    d = [(i[0], i[2]) for i in edges]\n",
    "\n",
    "    # Dictionary to store the sum and count of values for each name\n",
    "    sums_counts = {}\n",
    "\n",
    "    # Iterate through the list\n",
    "    for name, value in d:\n",
    "        if name in sums_counts:\n",
    "            sums_counts[name]['sum'] += value\n",
    "            sums_counts[name]['count'] += 1\n",
    "        else:\n",
    "            sums_counts[name] = {'sum': value, 'count': 1}\n",
    "\n",
    "    # Calculate the average for each name and store in a DataFrame\n",
    "    averages = pd.DataFrame.from_dict(sums_counts, orient='index')\n",
    "    #averages['average'] = averages['sum'] / averages['count']\n",
    "    averages.reset_index(inplace=True)\n",
    "    averages.rename(columns={'index': 'name'}, inplace=True)\n",
    "\n",
    "    # Add additional columns for country and house\n",
    "    averages['country'] = ''.join([char for char in edgesFile if char.isupper()])\n",
    "    averages['house'] = edgesFile[-15:-12]\n",
    "    averages['years'] = edgesFile[-12:-4]\n",
    "\n",
    "    # Select relevant columns\n",
    "    averages = averages[['name', 'sum', 'country', 'house', 'years']]\n",
    "\n",
    "    # Concatenate to the influence DataFrame\n",
    "    influence = pd.concat([influence, averages], ignore_index=True)\n",
    "\n",
    "birthdays = []\n",
    "gender = []\n",
    "party = []\n",
    "party_status = []\n",
    "party_orientation = []\n",
    "body = []\n",
    "role = []\n",
    "mp = []\n",
    "minister = []\n",
    "name = []\n",
    "\n",
    "missingGender = pd.read_excel(f'C:\\\\Users\\\\soren\\\\Documents\\\\beyond_rep\\\\missingData\\\\missing_gender.xlsx')\n",
    "missingGender = missingGender[['country', 'Speaker_name', 'gender']]\n",
    "missingAge = pd.read_excel(f'C:\\\\Users\\\\soren\\\\Documents\\\\beyond_rep\\\\missingData\\\\missing_age.xlsx')\n",
    "\n",
    "for country in tqdm(np.unique(influence.country)):\n",
    "    influence_country = influence[influence['country'] == country]\n",
    "    if len(country) == 4:\n",
    "        d_country = pd.read_csv(f'C:\\\\Users\\\\soren\\\\Documents\\\\beyond_rep\\\\parlamint_csv\\\\{country[0:2] + '-' + country[2:]}.csv')\n",
    "    else:\n",
    "        d_country = pd.read_csv(f'C:\\\\Users\\\\soren\\\\Documents\\\\beyond_rep\\\\parlamint_csv\\\\{country}.csv')\n",
    "\n",
    "    # Adding missing gender\n",
    "    d_country = pd.merge(d_country, missingGender, on=['country', 'Speaker_name'], how='left')\n",
    "    d_country['Speaker_gender'] = [None if i == '-' else i for i in d_country['Speaker_gender']]\n",
    "    d_country['Speaker_gender'] = d_country['Speaker_gender'].combine_first(d_country['gender'])\n",
    "\n",
    "    # Adding missing birthdays\n",
    "    d_country = pd.merge(d_country, missingAge, on=['country', 'Speaker_name'], how='left')\n",
    "\n",
    "    d_country['Speaker_birth'] = [None if i == '-' else i for i in d_country['Speaker_birth']]\n",
    "    d_country['Speaker_birth'] = d_country['Speaker_birth'].combine_first(d_country['birth'])\n",
    "        \n",
    "    for i, row in influence_country.iterrows():\n",
    "        speaker_info = d_country[d_country.Speaker_name == row['name']].iloc[0]\n",
    "\n",
    "        birthdays.append(speaker_info.Speaker_birth)\n",
    "        gender.append(speaker_info.Speaker_gender)\n",
    "        party.append(speaker_info.Speaker_party)\n",
    "        party_status.append(speaker_info.Party_status)\n",
    "        party_orientation.append(speaker_info.Party_orientation)\n",
    "        body.append(speaker_info.Body)\n",
    "        role.append(speaker_info.Speaker_role)\n",
    "        mp.append(speaker_info.Speaker_MP)\n",
    "        minister.append(speaker_info.Speaker_minister)\n",
    "        name.append(speaker_info.Speaker_name)\n",
    "    \n",
    "d_temp = pd.DataFrame({'birthdays' : birthdays,\n",
    "                       'gender' : gender,\n",
    "                       'party' : party,\n",
    "                       'party_status' : party_status,\n",
    "                       'party_orientation' : party_orientation,\n",
    "                       'body' : body,\n",
    "                       'role' : role,\n",
    "                       'mp' : mp,\n",
    "                       'minister' : minister,\n",
    "                       'name' : name})\n",
    "\n",
    "d_temp = d_temp.drop_duplicates(subset='name', keep='first')\n",
    "\n",
    "\n",
    "influence.merge(d_temp, on = 'name').to_csv('influence\\\\influenceSum.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
